{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28c064b-e31f-485d-98a7-ff4b82721d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece google-api-python-client -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb43e94-4fa8-4174-b407-72f7ca12c73b",
   "metadata": {},
   "source": [
    "### Instead of using new tokenizer, we can merge llama or mistral tokenizer and extend with own trained tokenizer\n",
    "### This way we can do transfer learning without training from scratch since we keep the index of original embeddings and just extend the new total index\n",
    "### As gpu poor, this method can help reduce training time and save cost\n",
    "### Based on https://github.com/ymcui/Chinese-LLaMA-Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbdb76a8-a593-4944-ae96-99ec80579b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "from transformers import LlamaTokenizer,AutoTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "from tokenizers import Tokenizer\n",
    "import sentencepiece as spm\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e5bf94-b4c5-4c1e-bac9-1228d47ab6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "# base_tokenizer_dir = \"meta-llama/Llama-2-7b-hf\" # base tokenizer\n",
    "base_tokenizer_dir = \"mistralai/Mistral-7B-v0.1\" # base tokenizer\n",
    "\n",
    "sp_model_file = \"malaysia-ai/sentencepiece-tokenizer\" # steal tokenizer from\n",
    "# sp_model_file = \"malaysia-ai/bpe-tokenizer\" # steal tokenizer from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad53d64-236b-4ffa-a9c9-499688278ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c099fdfee8a44f0db00f7ee4ace3cbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir: './malaysia-ai_sentencepiece-tokenizer'\n"
     ]
    }
   ],
   "source": [
    "# Download model\n",
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=sp_model_file, revision=\"main\", local_dir=f\"./{sp_model_file.replace('/', '_')}\")\n",
    "\n",
    "print(f\"Model dir: './{sp_model_file.replace('/', '_')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b7ffd07-219c-4731-9519-be9333e99f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_dir,use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a2b788-bbda-4862-8635-63bd3988fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = Tokenizer.from_file(f\"./{sp_model_file.replace('/', '_')}/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80fbd363-ada3-4ce2-9cb4-3de7a8881c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(base_tokenizer.sp_model.serialized_model_proto())\n",
    "my_spm = sp_pb2_model.ModelProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "279b0d11-525f-4e1f-adca-859547a0f741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "# print number of tokens\n",
    "print(len(base_tokenizer))\n",
    "print(base_tokenizer.all_special_tokens)\n",
    "print(base_tokenizer.all_special_ids)\n",
    "print(base_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e34afed-2e75-477e-9a7b-cf8463d8263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:32000\n"
     ]
    }
   ],
   "source": [
    "## Add Malaysia tokens to LLaMA tokenizer\n",
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eac5722-18ed-4bb0-8f64-0820ae566dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model pieces: 54877\n"
     ]
    }
   ],
   "source": [
    "my_vocab = my_tokenizer.get_vocab() # Dict[str, int]\n",
    "\n",
    "for vocab in my_vocab.keys():\n",
    "    piece = vocab\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162a89da-452e-4f31-9718-70366b3cd6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malaysia-LLaMA tokenizer has been saved to merged_tokenizer_hf\n"
     ]
    }
   ],
   "source": [
    "output_sp_dir = 'merged_tokenizer_sp'\n",
    "output_hf_dir = 'merged_tokenizer_hf' # the path to save Malaysia-LLaMA tokenizer\n",
    "os.makedirs(output_sp_dir,exist_ok=True)\n",
    "with open(output_sp_dir+'/malaysia_llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "tokenizer = type(base_tokenizer)(vocab_file=output_sp_dir+'/malaysia_llama.model')\n",
    "\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "print(f\"Malaysia-LLaMA tokenizer has been saved to {output_hf_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d7b1f9-b717-4270-b248-a49a65428785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_dir)\n",
    "my_base_tokenizer = AutoTokenizer.from_pretrained(output_hf_dir)\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ff6ff5-f6f4-47d3-a4a7-0197d0447dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len just to proof the table 1 from their paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dea58e2e-e2ac-499c-9fb4-79da328259aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text:\n",
      " husein comel\n",
      "Normalized text:\n",
      " husein comel\n",
      "------------------------------------------------------------\n",
      "[1, 295, 1730, 262, 432, 301]\n",
      "[1, 4946, 38982, 35502]\n",
      "Tokenized by LLaMA tokenizer(5):         ['▁h', 'use', 'in', '▁com', 'el']\n",
      "Tokenized by Malaysia-LLaMA tokenizer(3):['▁hus', 'ein', '▁comel']\n",
      "Recover llama tokenizer(16):<s> husein comel\n",
      "Recover new   tokenizer(16):<s> husein comel\n",
      "------------------------------------------------------------\n",
      "Test text:\n",
      " husein cute\n",
      "Normalized text:\n",
      " husein cute\n",
      "------------------------------------------------------------\n",
      "[1, 295, 1730, 262, 17949]\n",
      "[1, 4946, 38982, 17949]\n",
      "Tokenized by LLaMA tokenizer(4):         ['▁h', 'use', 'in', '▁cute']\n",
      "Tokenized by Malaysia-LLaMA tokenizer(3):['▁hus', 'ein', '▁cute']\n",
      "Recover llama tokenizer(15):<s> husein cute\n",
      "Recover new   tokenizer(15):<s> husein cute\n",
      "------------------------------------------------------------\n",
      "Test text:\n",
      " حسين چوميل\n",
      "Normalized text:\n",
      " حسين چوميل\n",
      "------------------------------------------------------------\n",
      "[1, 28705, 29100, 29008, 28972, 28955, 28705, 30066, 28962, 28954, 28972, 28933]\n",
      "[1, 46703, 48689, 28955, 52032, 40474, 52468]\n",
      "Tokenized by LLaMA tokenizer(11):         ['▁', 'ح', 'س', 'ي', 'ن', '▁', 'چ', 'و', 'م', 'ي', 'ل']\n",
      "Tokenized by Malaysia-LLaMA tokenizer(6):['▁ح', 'سي', 'ن', '▁چ', 'وم', 'يل']\n",
      "Recover llama tokenizer(14):<s> حسين چوميل\n",
      "Recover new   tokenizer(14):<s> حسين چوميل\n",
      "------------------------------------------------------------\n",
      "Test text:\n",
      " 侯赛因很可爱\n",
      "Normalized text:\n",
      " 侯赛因很可爱\n",
      "------------------------------------------------------------\n",
      "[1, 28705, 231, 193, 178, 31281, 29657, 30111, 29052, 30731]\n",
      "[1, 28705, 231, 193, 178, 31281, 29657, 30111, 44696]\n",
      "Tokenized by LLaMA tokenizer(9):         ['▁', '<0xE4>', '<0xBE>', '<0xAF>', '赛', '因', '很', '可', '爱']\n",
      "Tokenized by Malaysia-LLaMA tokenizer(8):['▁', '<0xE4>', '<0xBE>', '<0xAF>', '赛', '因', '很', '可爱']\n",
      "Recover llama tokenizer(10):<s> 侯赛因很可爱\n",
      "Recover new   tokenizer(10):<s> 侯赛因很可爱\n",
      "------------------------------------------------------------\n",
      "Test text:\n",
      " ஹுசைன் அழகாக இருக்கிறார்\n",
      "Normalized text:\n",
      " ஹுசைன் அழகாக இருக்கிறார்\n",
      "------------------------------------------------------------\n",
      "[1, 28705, 227, 177, 188, 29981, 30934, 30576, 30804, 29431, 28705, 227, 177, 136, 227, 177, 183, 29856, 30419, 29856, 28705, 227, 177, 138, 30368, 29981, 29856, 29431, 29856, 29983, 31002, 30419, 30368, 29431]\n",
      "[1, 28705, 227, 177, 188, 29981, 30934, 30576, 30804, 29431, 28705, 227, 177, 136, 227, 177, 183, 29856, 30419, 29856, 28705, 227, 177, 138, 30368, 29981, 37176, 29856, 29983, 31002, 30419, 30368, 29431]\n",
      "Tokenized by LLaMA tokenizer(33):         ['▁', '<0xE0>', '<0xAE>', '<0xB9>', 'ு', 'ச', 'ை', 'ன', '்', '▁', '<0xE0>', '<0xAE>', '<0x85>', '<0xE0>', '<0xAE>', '<0xB4>', 'க', 'ா', 'க', '▁', '<0xE0>', '<0xAE>', '<0x87>', 'ர', 'ு', 'க', '்', 'க', 'ி', 'ற', 'ா', 'ர', '்']\n",
      "Tokenized by Malaysia-LLaMA tokenizer(32):['▁', '<0xE0>', '<0xAE>', '<0xB9>', 'ு', 'ச', 'ை', 'ன', '்', '▁', '<0xE0>', '<0xAE>', '<0x85>', '<0xE0>', '<0xAE>', '<0xB4>', 'க', 'ா', 'க', '▁', '<0xE0>', '<0xAE>', '<0x87>', 'ர', 'ு', 'க்', 'க', 'ி', 'ற', 'ா', 'ர', '்']\n",
      "Recover llama tokenizer(28):<s> ஹுசைன் அழகாக இருக்கிறார்\n",
      "Recover new   tokenizer(28):<s> ஹுசைன் அழகாக இருக்கிறார்\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"husein comel\",\n",
    "    \"husein cute\",\n",
    "    \"حسين چوميل\",\n",
    "    \"侯赛因很可爱\",\n",
    "    \"ஹுசைன் அழகாக இருக்கிறார்\"\n",
    "]\n",
    "for text in texts:\n",
    "    print(\"Test text:\\n\",text)\n",
    "    norm_text = unicodedata.normalize('NFKC', text)\n",
    "    print(\"Normalized text:\\n\", norm_text)\n",
    "    print(\"---\"*20)\n",
    "    og = base_tokenizer.tokenize(norm_text)\n",
    "    og_ids = base_tokenizer.encode(norm_text)\n",
    "    recoverog = base_tokenizer.decode(og_ids)\n",
    "    \n",
    "    nw = my_base_tokenizer.tokenize(norm_text)\n",
    "    nw_ids = my_base_tokenizer.encode(norm_text)\n",
    "    print(og_ids)\n",
    "    print(nw_ids)\n",
    "    recovernw = my_base_tokenizer.decode(nw_ids)\n",
    "    print(f\"Tokenized by LLaMA tokenizer({len(og)}):         {og}\")\n",
    "    print(f\"Tokenized by Malaysia-LLaMA tokenizer({len(nw)}):{nw}\")\n",
    "    print(f\"Recover llama tokenizer({len(recoverog)}):{recoverog}\")\n",
    "    print(f\"Recover new   tokenizer({len(recovernw)}):{recovernw}\")\n",
    "    print(\"---\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b3786-3dd7-4649-874b-2a2ed917745a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
